{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-06-04T13:31:50.308311Z",
     "start_time": "2025-06-04T13:31:49.644785Z"
    }
   },
   "source": [
    "import torch\n",
    "from GPTModel import GPTModel\n",
    "\n",
    "GPT_CONFIG_124M = {\n",
    "    \"vocab_size\": 50257,\n",
    "    \"context_length\": 256,    #1\n",
    "    \"emb_dim\": 768,\n",
    "    \"n_heads\": 12,\n",
    "    \"n_layers\": 12,\n",
    "    \"drop_rate\": 0.1,       #2\n",
    "    \"qkv_bias\": False\n",
    "}\n",
    "torch.manual_seed(123)\n",
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "model.eval()"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPTModel(\n",
       "  (tok_emb): Embedding(50257, 768)\n",
       "  (pos_emb): Embedding(256, 768)\n",
       "  (drop_emb): Dropout(p=0.1, inplace=False)\n",
       "  (trf_blocks): Sequential(\n",
       "    (0): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (1): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (2): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (3): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (4): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (5): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (6): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (7): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (8): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (9): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (10): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (11): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (final_norm): LayerNorm()\n",
       "  (out_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-04T13:32:54.925947Z",
     "start_time": "2025-06-04T13:32:54.408693Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import tiktoken\n",
    "from GenerateTextSimple import generate_text_simple\n",
    "\n",
    "def text_to_token_ids(text, tokenizer):\n",
    "    encoded = tokenizer.encode(text, allowed_special={'<|endoftext|>'})\n",
    "    encoded_tensor = torch.tensor(encoded).unsqueeze(0)    #1\n",
    "    return encoded_tensor\n",
    "\n",
    "def token_ids_to_text(token_ids, tokenizer):\n",
    "    flat = token_ids.squeeze(0)                #2\n",
    "    return tokenizer.decode(flat.tolist())\n",
    "\n",
    "start_context = \"Every effort moves you\"\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "token_ids = generate_text_simple(\n",
    "    model=model,\n",
    "    idx=text_to_token_ids(start_context, tokenizer),\n",
    "    max_new_tokens=10,\n",
    "    context_size=GPT_CONFIG_124M[\"context_length\"]\n",
    ")\n",
    "print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))"
   ],
   "id": "53b70a28b7cb0c3b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output text:\n",
      " Every effort moves you rentingetic wasnم refres RexMeCHicular stren\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-04T13:33:04.514304Z",
     "start_time": "2025-06-04T13:33:04.512431Z"
    }
   },
   "cell_type": "code",
   "source": [
    "inputs = torch.tensor([[16833, 3626, 6100],   # [\"every effort moves\",\n",
    "                       [40,    1107, 588]])   #  \"I really like\"]"
   ],
   "id": "4b986f04fb7e4976",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-04T13:33:11.104356Z",
     "start_time": "2025-06-04T13:33:11.102294Z"
    }
   },
   "cell_type": "code",
   "source": [
    "targets = torch.tensor([[3626, 6100, 345  ],  # [\" effort moves you\",\n",
    "                        [1107, 588, 11311]])  #  \" really like chocolate\"]"
   ],
   "id": "dc31933d81154e46",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-04T13:33:17.198660Z",
     "start_time": "2025-06-04T13:33:17.165467Z"
    }
   },
   "cell_type": "code",
   "source": [
    "with torch.no_grad():     #1\n",
    "    logits = model(inputs)\n",
    "probas = torch.softmax(logits, dim=-1)     #2\n",
    "print(probas.shape)"
   ],
   "id": "1d222608e93825ff",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 50257])\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-04T13:33:23.996723Z",
     "start_time": "2025-06-04T13:33:23.994362Z"
    }
   },
   "cell_type": "code",
   "source": [
    "token_ids = torch.argmax(probas, dim=-1, keepdim=True)\n",
    "print(\"Token IDs:\\n\", token_ids)"
   ],
   "id": "2351e675d7e09720",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token IDs:\n",
      " tensor([[[16657],\n",
      "         [  339],\n",
      "         [42826]],\n",
      "\n",
      "        [[49906],\n",
      "         [29669],\n",
      "         [41751]]])\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-04T13:33:30.194914Z",
     "start_time": "2025-06-04T13:33:30.192809Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(f\"Targets batch 1: {token_ids_to_text(targets[0], tokenizer)}\")\n",
    "print(f\"Outputs batch 1:\"\n",
    "      f\" {token_ids_to_text(token_ids[0].flatten(), tokenizer)}\")"
   ],
   "id": "3c2ccc56a87308f1",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Targets batch 1:  effort moves you\n",
      "Outputs batch 1:  Armed heNetflix\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-04T13:33:38.026626Z",
     "start_time": "2025-06-04T13:33:38.023889Z"
    }
   },
   "cell_type": "code",
   "source": [
    "text_idx = 0\n",
    "target_probas_1 = probas[text_idx, [0, 1, 2], targets[text_idx]]\n",
    "print(\"Text 1:\", target_probas_1)\n",
    "\n",
    "text_idx = 1\n",
    "target_probas_2 = probas[text_idx, [0, 1, 2], targets[text_idx]]\n",
    "print(\"Text 2:\", target_probas_2)"
   ],
   "id": "d70d50cf923883fa",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text 1: tensor([7.4540e-05, 3.1061e-05, 1.1563e-05])\n",
      "Text 2: tensor([1.0337e-05, 5.6776e-05, 4.7559e-06])\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-04T13:33:43.647652Z",
     "start_time": "2025-06-04T13:33:43.645252Z"
    }
   },
   "cell_type": "code",
   "source": [
    "log_probas = torch.log(torch.cat((target_probas_1, target_probas_2)))\n",
    "print(log_probas)"
   ],
   "id": "ff180c2ec6baa97e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ -9.5042, -10.3796, -11.3677, -11.4798,  -9.7764, -12.2561])\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-04T13:33:49.523751Z",
     "start_time": "2025-06-04T13:33:49.521462Z"
    }
   },
   "cell_type": "code",
   "source": [
    "avg_log_probas = torch.mean(log_probas)\n",
    "print(avg_log_probas)"
   ],
   "id": "17d67081910eb6bf",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-10.7940)\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-04T13:33:55.746738Z",
     "start_time": "2025-06-04T13:33:55.744556Z"
    }
   },
   "cell_type": "code",
   "source": [
    "neg_avg_log_probas = avg_log_probas * -1\n",
    "print(neg_avg_log_probas)"
   ],
   "id": "11d2450aa373c30b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(10.7940)\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-04T13:34:01.125124Z",
     "start_time": "2025-06-04T13:34:01.123025Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(\"Logits shape:\", logits.shape)\n",
    "print(\"Targets shape:\", targets.shape)"
   ],
   "id": "432bf49777cd60e8",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logits shape: torch.Size([2, 3, 50257])\n",
      "Targets shape: torch.Size([2, 3])\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-04T13:34:06.906341Z",
     "start_time": "2025-06-04T13:34:06.904371Z"
    }
   },
   "cell_type": "code",
   "source": [
    "logits_flat = logits.flatten(0, 1)\n",
    "targets_flat = targets.flatten()\n",
    "print(\"Flattened logits:\", logits_flat.shape)\n",
    "print(\"Flattened targets:\", targets_flat.shape)"
   ],
   "id": "5a8d3a11ae3ecbe",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Flattened logits: torch.Size([6, 50257])\n",
      "Flattened targets: torch.Size([6])\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-04T13:34:12.637788Z",
     "start_time": "2025-06-04T13:34:12.635472Z"
    }
   },
   "cell_type": "code",
   "source": [
    "loss = torch.nn.functional.cross_entropy(logits_flat, targets_flat)\n",
    "print(loss)"
   ],
   "id": "4ebad5711e4557d2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(10.7940)\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-04T13:35:46.079949Z",
     "start_time": "2025-06-04T13:35:46.078045Z"
    }
   },
   "cell_type": "code",
   "source": [
    "file_path = \"files/the-verdict.txt\"\n",
    "with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "    text_data = file.read()"
   ],
   "id": "ca84c3a0b5e0cf6",
   "outputs": [],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-04T13:35:46.879136Z",
     "start_time": "2025-06-04T13:35:46.862507Z"
    }
   },
   "cell_type": "code",
   "source": [
    "total_characters = len(text_data)\n",
    "total_tokens = len(tokenizer.encode(text_data))\n",
    "print(\"Characters:\", total_characters)\n",
    "print(\"Tokens:\", total_tokens)"
   ],
   "id": "d73a4329ef5c358e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Characters: 226961\n",
      "Tokens: 135982\n"
     ]
    }
   ],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-04T13:36:00.747840Z",
     "start_time": "2025-06-04T13:36:00.745998Z"
    }
   },
   "cell_type": "code",
   "source": [
    "train_ratio = 0.90\n",
    "split_idx = int(train_ratio * len(text_data))\n",
    "train_data = text_data[:split_idx]\n",
    "val_data = text_data[split_idx:]"
   ],
   "id": "d89dd3f8ecbe40d8",
   "outputs": [],
   "execution_count": 22
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-04T13:37:12.283867Z",
     "start_time": "2025-06-04T13:37:12.233125Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from CreateDataLoader import create_dataloader_v1\n",
    "\n",
    "torch.manual_seed(123)\n",
    "\n",
    "train_loader = create_dataloader_v1(\n",
    "    train_data,\n",
    "    batch_size=2,\n",
    "    max_length=GPT_CONFIG_124M[\"context_length\"],\n",
    "    stride=GPT_CONFIG_124M[\"context_length\"],\n",
    "    drop_last=True,\n",
    "    shuffle=True,\n",
    "    num_workers=0\n",
    ")\n",
    "val_loader = create_dataloader_v1(\n",
    "    val_data,\n",
    "    batch_size=2,\n",
    "    max_length=GPT_CONFIG_124M[\"context_length\"],\n",
    "    stride=GPT_CONFIG_124M[\"context_length\"],\n",
    "    drop_last=False,\n",
    "    shuffle=False,\n",
    "    num_workers=0\n",
    ")"
   ],
   "id": "24e879a30b3b5653",
   "outputs": [],
   "execution_count": 23
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-04T13:37:30.080304Z",
     "start_time": "2025-06-04T13:37:30.068453Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(\"Train loader:\")\n",
    "for x, y in train_loader:\n",
    "    print(x.shape, y.shape)\n",
    "\n",
    "print(\"\\nValidation loader:\")\n",
    "for x, y in val_loader:\n",
    "    print(x.shape, y.shape)"
   ],
   "id": "ee1624e9b0bf9c41",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loader:\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "\n",
      "Validation loader:\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([1, 256]) torch.Size([1, 256])\n"
     ]
    }
   ],
   "execution_count": 24
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-04T13:37:37.131493Z",
     "start_time": "2025-06-04T13:37:37.129224Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def calc_loss_batch(input_batch, target_batch, model, device):\n",
    "    input_batch = input_batch.to(device)         #1\n",
    "    target_batch = target_batch.to(device)\n",
    "    logits = model(input_batch)\n",
    "    loss = torch.nn.functional.cross_entropy(\n",
    "        logits.flatten(0, 1), target_batch.flatten()\n",
    "    )\n",
    "    return loss"
   ],
   "id": "41779c4ed2e7414f",
   "outputs": [],
   "execution_count": 25
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-04T13:37:42.907086Z",
     "start_time": "2025-06-04T13:37:42.904657Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def calc_loss_loader(data_loader, model, device, num_batches=None):\n",
    "    total_loss = 0.\n",
    "    if len(data_loader) == 0:\n",
    "        return float(\"nan\")\n",
    "    elif num_batches is None:\n",
    "        num_batches = len(data_loader)     #1\n",
    "    else:\n",
    "        num_batches = min(num_batches, len(data_loader))   #2\n",
    "    for i, (input_batch, target_batch) in enumerate(data_loader):\n",
    "        if i < num_batches:\n",
    "            loss = calc_loss_batch(\n",
    "                input_batch, target_batch, model, device\n",
    "            )\n",
    "            total_loss += loss.item()    #3\n",
    "        else:\n",
    "            break\n",
    "    return total_loss / num_batches    #4"
   ],
   "id": "6126e84ab269d8cc",
   "outputs": [],
   "execution_count": 26
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-04T13:37:51.478150Z",
     "start_time": "2025-06-04T13:37:48.085916Z"
    }
   },
   "cell_type": "code",
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)   #1\n",
    "with torch.no_grad():                                        #2\n",
    "    train_loss = calc_loss_loader(train_loader, model, device)    #3\n",
    "    val_loss = calc_loss_loader(val_loader, model, device)\n",
    "print(\"Training loss:\", train_loss)\n",
    "print(\"Validation loss:\", val_loss)"
   ],
   "id": "5b25a7cb393e658e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 11.355853016636953\n",
      "Validation loss: 11.372389263576931\n"
     ]
    }
   ],
   "execution_count": 27
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-04T13:38:26.715289Z",
     "start_time": "2025-06-04T13:38:26.713123Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def evaluate_model(model, train_loader, val_loader, device, eval_iter):\n",
    "    model.eval()  #1\n",
    "    with torch.no_grad():                              #2\n",
    "        train_loss = calc_loss_loader(\n",
    "            train_loader, model, device, num_batches=eval_iter\n",
    "        )\n",
    "        val_loss = calc_loss_loader(\n",
    "            val_loader, model, device, num_batches=eval_iter\n",
    "        )\n",
    "    model.train()\n",
    "    return train_loss, val_loss"
   ],
   "id": "20a456b81b803143",
   "outputs": [],
   "execution_count": 29
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-04T13:38:35.159271Z",
     "start_time": "2025-06-04T13:38:35.156424Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def generate_and_print_sample(model, tokenizer, device, start_context):\n",
    "    model.eval()\n",
    "    context_size = model.pos_emb.weight.shape[0]\n",
    "    encoded = text_to_token_ids(start_context, tokenizer).to(device)\n",
    "    with torch.no_grad():\n",
    "        token_ids = generate_text_simple(\n",
    "            model=model, idx=encoded,\n",
    "            max_new_tokens=50, context_size=context_size\n",
    "        )\n",
    "    decoded_text = token_ids_to_text(token_ids, tokenizer)\n",
    "    print(decoded_text.replace(\"\\n\", \" \"))      #1\n",
    "    model.train()"
   ],
   "id": "63b8f167ea6a1f48",
   "outputs": [],
   "execution_count": 30
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-04T13:38:36.445721Z",
     "start_time": "2025-06-04T13:38:36.442251Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def train_model_simple(model, train_loader, val_loader,\n",
    "                       optimizer, device, num_epochs,\n",
    "                       eval_freq, eval_iter, start_context, tokenizer):\n",
    "    train_losses, val_losses, track_tokens_seen = [], [], []    #1\n",
    "    tokens_seen, global_step = 0, -1\n",
    "\n",
    "    for epoch in range(num_epochs):    #2\n",
    "        model.train()\n",
    "        for input_batch, target_batch in train_loader:\n",
    "            optimizer.zero_grad()   #3\n",
    "            loss = calc_loss_batch(\n",
    "                input_batch, target_batch, model, device\n",
    "            )\n",
    "            loss.backward()                     #4\n",
    "            optimizer.step()                    #5\n",
    "            tokens_seen += input_batch.numel()\n",
    "            global_step += 1\n",
    "\n",
    "            if global_step % eval_freq == 0:    #6\n",
    "                train_loss, val_loss = evaluate_model(\n",
    "                    model, train_loader, val_loader, device, eval_iter)\n",
    "                train_losses.append(train_loss)\n",
    "                val_losses.append(val_loss)\n",
    "                track_tokens_seen.append(tokens_seen)\n",
    "                print(f\"Ep {epoch+1} (Step {global_step:06d}): \"\n",
    "                      f\"Train loss {train_loss:.3f}, \"\n",
    "                      f\"Val loss {val_loss:.3f}\"\n",
    "                )\n",
    "\n",
    "        generate_and_print_sample(                      #7\n",
    "            model, tokenizer, device, start_context\n",
    "        )\n",
    "    return train_losses, val_losses, track_tokens_seen"
   ],
   "id": "9c75b71e7197d499",
   "outputs": [],
   "execution_count": 31
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-04T13:42:21.195973Z",
     "start_time": "2025-06-04T13:38:45.031043Z"
    }
   },
   "cell_type": "code",
   "source": [
    "torch.manual_seed(123)\n",
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "model.to(device)\n",
    "optimizer = torch.optim.AdamW(\n",
    "     model.parameters(),           #1\n",
    "    lr=0.0004, weight_decay=0.1\n",
    ")\n",
    "num_epochs = 10\n",
    "train_losses, val_losses, tokens_seen = train_model_simple(\n",
    "    model, train_loader, val_loader, optimizer, device,\n",
    "    num_epochs=num_epochs, eval_freq=5, eval_iter=5,\n",
    "    start_context=\"Every effort moves you\", tokenizer=tokenizer\n",
    ")"
   ],
   "id": "7e642d835e516851",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 1 (Step 000000): Train loss 3.691, Val loss 2.879\n",
      "Ep 1 (Step 000005): Train loss 3.208, Val loss 3.051\n",
      "Ep 1 (Step 000010): Train loss 2.939, Val loss 2.399\n",
      "Ep 1 (Step 000015): Train loss 2.214, Val loss 1.979\n",
      "Ep 1 (Step 000020): Train loss 2.233, Val loss 1.837\n",
      "Ep 1 (Step 000025): Train loss 1.930, Val loss 1.772\n",
      "Ep 1 (Step 000030): Train loss 2.193, Val loss 1.737\n",
      "Ep 1 (Step 000035): Train loss 1.867, Val loss 1.707\n",
      "Ep 1 (Step 000040): Train loss 1.714, Val loss 1.685\n",
      "Ep 1 (Step 000045): Train loss 2.203, Val loss 1.659\n",
      "Ep 1 (Step 000050): Train loss 1.931, Val loss 1.646\n",
      "Ep 1 (Step 000055): Train loss 1.928, Val loss 1.640\n",
      "Ep 1 (Step 000060): Train loss 1.794, Val loss 1.630\n",
      "Ep 1 (Step 000065): Train loss 1.452, Val loss 1.615\n",
      "Ep 1 (Step 000070): Train loss 2.567, Val loss 1.602\n",
      "Ep 1 (Step 000075): Train loss 1.941, Val loss 1.605\n",
      "Ep 1 (Step 000080): Train loss 1.758, Val loss 1.597\n",
      "Ep 1 (Step 000085): Train loss 1.682, Val loss 1.591\n",
      "Ep 1 (Step 000090): Train loss 1.789, Val loss 1.569\n",
      "Ep 1 (Step 000095): Train loss 1.828, Val loss 1.538\n",
      "Ep 1 (Step 000100): Train loss 1.881, Val loss 1.522\n",
      "Ep 1 (Step 000105): Train loss 2.304, Val loss 1.518\n",
      "Ep 1 (Step 000110): Train loss 1.573, Val loss 1.501\n",
      "Ep 1 (Step 000115): Train loss 1.485, Val loss 1.501\n",
      "Ep 1 (Step 000120): Train loss 1.718, Val loss 1.489\n",
      "Ep 1 (Step 000125): Train loss 1.514, Val loss 1.482\n",
      "Ep 1 (Step 000130): Train loss 1.577, Val loss 1.480\n",
      "Ep 1 (Step 000135): Train loss 2.124, Val loss 1.474\n",
      "Ep 1 (Step 000140): Train loss 1.973, Val loss 1.485\n",
      "Ep 1 (Step 000145): Train loss 2.018, Val loss 1.504\n",
      "Ep 1 (Step 000150): Train loss 1.926, Val loss 1.466\n",
      "Ep 1 (Step 000155): Train loss 2.176, Val loss 1.463\n",
      "Ep 1 (Step 000160): Train loss 1.846, Val loss 1.478\n",
      "Ep 1 (Step 000165): Train loss 1.407, Val loss 1.476\n",
      "Ep 1 (Step 000170): Train loss 1.683, Val loss 1.464\n",
      "Ep 1 (Step 000175): Train loss 1.458, Val loss 1.460\n",
      "Ep 1 (Step 000180): Train loss 1.472, Val loss 1.467\n",
      "Ep 1 (Step 000185): Train loss 1.391, Val loss 1.447\n",
      "Ep 1 (Step 000190): Train loss 1.646, Val loss 1.453\n",
      "Ep 1 (Step 000195): Train loss 1.843, Val loss 1.462\n",
      "Ep 1 (Step 000200): Train loss 1.772, Val loss 1.452\n",
      "Ep 1 (Step 000205): Train loss 1.899, Val loss 1.456\n",
      "Ep 1 (Step 000210): Train loss 1.647, Val loss 1.439\n",
      "Ep 1 (Step 000215): Train loss 1.478, Val loss 1.437\n",
      "Ep 1 (Step 000220): Train loss 1.530, Val loss 1.433\n",
      "Ep 1 (Step 000225): Train loss 1.509, Val loss 1.432\n",
      "Ep 1 (Step 000230): Train loss 1.877, Val loss 1.438\n",
      "Ep 1 (Step 000235): Train loss 1.372, Val loss 1.436\n",
      "Every effort moves you,                                                 \n",
      "Ep 2 (Step 000240): Train loss 1.311, Val loss 1.420\n",
      "Ep 2 (Step 000245): Train loss 1.561, Val loss 1.398\n",
      "Ep 2 (Step 000250): Train loss 1.686, Val loss 1.411\n",
      "Ep 2 (Step 000255): Train loss 1.461, Val loss 1.399\n",
      "Ep 2 (Step 000260): Train loss 1.790, Val loss 1.398\n",
      "Ep 2 (Step 000265): Train loss 1.512, Val loss 1.388\n",
      "Ep 2 (Step 000270): Train loss 1.847, Val loss 1.392\n",
      "Ep 2 (Step 000275): Train loss 1.407, Val loss 1.379\n",
      "Ep 2 (Step 000280): Train loss 1.568, Val loss 1.361\n",
      "Ep 2 (Step 000285): Train loss 1.444, Val loss 1.352\n",
      "Ep 2 (Step 000290): Train loss 1.632, Val loss 1.329\n",
      "Ep 2 (Step 000295): Train loss 1.485, Val loss 1.336\n",
      "Ep 2 (Step 000300): Train loss 1.662, Val loss 1.386\n",
      "Ep 2 (Step 000305): Train loss 2.087, Val loss 1.348\n",
      "Ep 2 (Step 000310): Train loss 1.543, Val loss 1.328\n",
      "Ep 2 (Step 000315): Train loss 1.525, Val loss 1.336\n",
      "Ep 2 (Step 000320): Train loss 1.561, Val loss 1.317\n",
      "Ep 2 (Step 000325): Train loss 1.274, Val loss 1.320\n",
      "Ep 2 (Step 000330): Train loss 1.477, Val loss 1.314\n",
      "Ep 2 (Step 000335): Train loss 1.714, Val loss 1.304\n",
      "Ep 2 (Step 000340): Train loss 1.225, Val loss 1.299\n",
      "Ep 2 (Step 000345): Train loss 1.160, Val loss 1.297\n",
      "Ep 2 (Step 000350): Train loss 1.447, Val loss 1.301\n",
      "Ep 2 (Step 000355): Train loss 1.265, Val loss 1.295\n",
      "Ep 2 (Step 000360): Train loss 1.464, Val loss 1.315\n",
      "Ep 2 (Step 000365): Train loss 1.497, Val loss 1.316\n",
      "Ep 2 (Step 000370): Train loss 1.465, Val loss 1.314\n",
      "Ep 2 (Step 000375): Train loss 1.119, Val loss 1.321\n",
      "Ep 2 (Step 000380): Train loss 1.738, Val loss 1.298\n",
      "Ep 2 (Step 000385): Train loss 1.261, Val loss 1.286\n",
      "Ep 2 (Step 000390): Train loss 1.387, Val loss 1.273\n",
      "Ep 2 (Step 000395): Train loss 1.419, Val loss 1.275\n",
      "Ep 2 (Step 000400): Train loss 1.129, Val loss 1.262\n",
      "Ep 2 (Step 000405): Train loss 1.626, Val loss 1.282\n",
      "Ep 2 (Step 000410): Train loss 1.378, Val loss 1.271\n",
      "Ep 2 (Step 000415): Train loss 1.357, Val loss 1.259\n",
      "Ep 2 (Step 000420): Train loss 1.252, Val loss 1.256\n",
      "Ep 2 (Step 000425): Train loss 1.407, Val loss 1.273\n",
      "Ep 2 (Step 000430): Train loss 1.216, Val loss 1.274\n",
      "Ep 2 (Step 000435): Train loss 1.824, Val loss 1.257\n",
      "Ep 2 (Step 000440): Train loss 1.239, Val loss 1.258\n",
      "Ep 2 (Step 000445): Train loss 1.408, Val loss 1.257\n",
      "Ep 2 (Step 000450): Train loss 1.350, Val loss 1.265\n",
      "Ep 2 (Step 000455): Train loss 1.110, Val loss 1.262\n",
      "Ep 2 (Step 000460): Train loss 1.208, Val loss 1.245\n",
      "Ep 2 (Step 000465): Train loss 1.209, Val loss 1.260\n",
      "Ep 2 (Step 000470): Train loss 1.100, Val loss 1.247\n",
      "Ep 2 (Step 000475): Train loss 1.184, Val loss 1.249\n",
      "Every effort moves you're                                                 \n",
      "Ep 3 (Step 000480): Train loss 1.079, Val loss 1.259\n",
      "Ep 3 (Step 000485): Train loss 1.055, Val loss 1.254\n",
      "Ep 3 (Step 000490): Train loss 1.140, Val loss 1.237\n",
      "Ep 3 (Step 000495): Train loss 1.223, Val loss 1.246\n",
      "Ep 3 (Step 000500): Train loss 1.135, Val loss 1.260\n",
      "Ep 3 (Step 000505): Train loss 1.083, Val loss 1.248\n",
      "Ep 3 (Step 000510): Train loss 1.005, Val loss 1.256\n",
      "Ep 3 (Step 000515): Train loss 1.269, Val loss 1.259\n",
      "Ep 3 (Step 000520): Train loss 1.359, Val loss 1.241\n",
      "Ep 3 (Step 000525): Train loss 1.214, Val loss 1.239\n",
      "Ep 3 (Step 000530): Train loss 1.320, Val loss 1.225\n",
      "Ep 3 (Step 000535): Train loss 1.173, Val loss 1.234\n",
      "Ep 3 (Step 000540): Train loss 1.067, Val loss 1.242\n",
      "Ep 3 (Step 000545): Train loss 1.099, Val loss 1.231\n",
      "Ep 3 (Step 000550): Train loss 1.307, Val loss 1.219\n",
      "Ep 3 (Step 000555): Train loss 1.460, Val loss 1.217\n",
      "Ep 3 (Step 000560): Train loss 1.031, Val loss 1.228\n",
      "Ep 3 (Step 000565): Train loss 1.146, Val loss 1.240\n",
      "Ep 3 (Step 000570): Train loss 1.167, Val loss 1.234\n",
      "Ep 3 (Step 000575): Train loss 1.085, Val loss 1.230\n",
      "Ep 3 (Step 000580): Train loss 1.281, Val loss 1.228\n",
      "Ep 3 (Step 000585): Train loss 0.925, Val loss 1.242\n",
      "Ep 3 (Step 000590): Train loss 1.043, Val loss 1.219\n",
      "Ep 3 (Step 000595): Train loss 1.065, Val loss 1.201\n",
      "Ep 3 (Step 000600): Train loss 1.346, Val loss 1.197\n",
      "Ep 3 (Step 000605): Train loss 1.110, Val loss 1.208\n",
      "Ep 3 (Step 000610): Train loss 0.977, Val loss 1.221\n",
      "Ep 3 (Step 000615): Train loss 1.016, Val loss 1.211\n",
      "Ep 3 (Step 000620): Train loss 1.282, Val loss 1.200\n",
      "Ep 3 (Step 000625): Train loss 1.152, Val loss 1.210\n",
      "Ep 3 (Step 000630): Train loss 1.011, Val loss 1.212\n",
      "Ep 3 (Step 000635): Train loss 1.054, Val loss 1.201\n",
      "Ep 3 (Step 000640): Train loss 1.165, Val loss 1.205\n",
      "Ep 3 (Step 000645): Train loss 1.056, Val loss 1.202\n",
      "Ep 3 (Step 000650): Train loss 1.122, Val loss 1.203\n",
      "Ep 3 (Step 000655): Train loss 0.977, Val loss 1.208\n",
      "Ep 3 (Step 000660): Train loss 1.005, Val loss 1.213\n",
      "Ep 3 (Step 000665): Train loss 1.114, Val loss 1.214\n",
      "Ep 3 (Step 000670): Train loss 1.064, Val loss 1.208\n",
      "Ep 3 (Step 000675): Train loss 1.036, Val loss 1.207\n",
      "Ep 3 (Step 000680): Train loss 0.906, Val loss 1.202\n",
      "Ep 3 (Step 000685): Train loss 1.266, Val loss 1.192\n",
      "Ep 3 (Step 000690): Train loss 0.867, Val loss 1.207\n",
      "Ep 3 (Step 000695): Train loss 1.165, Val loss 1.204\n",
      "Ep 3 (Step 000700): Train loss 0.894, Val loss 1.196\n",
      "Ep 3 (Step 000705): Train loss 0.850, Val loss 1.199\n",
      "Ep 3 (Step 000710): Train loss 1.019, Val loss 1.201\n",
      "Every effort moves you,                                                 \n",
      "Ep 4 (Step 000715): Train loss 1.007, Val loss 1.193\n",
      "Ep 4 (Step 000720): Train loss 1.039, Val loss 1.180\n",
      "Ep 4 (Step 000725): Train loss 0.955, Val loss 1.182\n",
      "Ep 4 (Step 000730): Train loss 0.825, Val loss 1.191\n",
      "Ep 4 (Step 000735): Train loss 1.198, Val loss 1.177\n",
      "Ep 4 (Step 000740): Train loss 0.862, Val loss 1.183\n",
      "Ep 4 (Step 000745): Train loss 0.878, Val loss 1.198\n",
      "Ep 4 (Step 000750): Train loss 0.921, Val loss 1.196\n",
      "Ep 4 (Step 000755): Train loss 0.921, Val loss 1.193\n",
      "Ep 4 (Step 000760): Train loss 0.938, Val loss 1.186\n",
      "Ep 4 (Step 000765): Train loss 0.922, Val loss 1.194\n",
      "Ep 4 (Step 000770): Train loss 0.927, Val loss 1.203\n",
      "Ep 4 (Step 000775): Train loss 0.849, Val loss 1.203\n",
      "Ep 4 (Step 000780): Train loss 0.770, Val loss 1.197\n",
      "Ep 4 (Step 000785): Train loss 0.936, Val loss 1.200\n",
      "Ep 4 (Step 000790): Train loss 0.947, Val loss 1.220\n",
      "Ep 4 (Step 000795): Train loss 0.921, Val loss 1.219\n",
      "Ep 4 (Step 000800): Train loss 0.691, Val loss 1.212\n",
      "Ep 4 (Step 000805): Train loss 0.783, Val loss 1.218\n",
      "Ep 4 (Step 000810): Train loss 1.013, Val loss 1.222\n",
      "Ep 4 (Step 000815): Train loss 0.863, Val loss 1.203\n",
      "Ep 4 (Step 000820): Train loss 1.088, Val loss 1.214\n",
      "Ep 4 (Step 000825): Train loss 0.764, Val loss 1.223\n",
      "Ep 4 (Step 000830): Train loss 0.861, Val loss 1.227\n",
      "Ep 4 (Step 000835): Train loss 0.946, Val loss 1.237\n",
      "Ep 4 (Step 000840): Train loss 0.778, Val loss 1.239\n",
      "Ep 4 (Step 000845): Train loss 0.814, Val loss 1.224\n",
      "Ep 4 (Step 000850): Train loss 0.849, Val loss 1.228\n",
      "Ep 4 (Step 000855): Train loss 0.829, Val loss 1.237\n",
      "Ep 4 (Step 000860): Train loss 0.980, Val loss 1.217\n",
      "Ep 4 (Step 000865): Train loss 0.802, Val loss 1.214\n",
      "Ep 4 (Step 000870): Train loss 0.888, Val loss 1.200\n",
      "Ep 4 (Step 000875): Train loss 0.985, Val loss 1.186\n",
      "Ep 4 (Step 000880): Train loss 0.738, Val loss 1.181\n",
      "Ep 4 (Step 000885): Train loss 0.923, Val loss 1.181\n",
      "Ep 4 (Step 000890): Train loss 0.874, Val loss 1.196\n",
      "Ep 4 (Step 000895): Train loss 0.726, Val loss 1.206\n",
      "Ep 4 (Step 000900): Train loss 0.693, Val loss 1.209\n",
      "Ep 4 (Step 000905): Train loss 0.946, Val loss 1.225\n",
      "Ep 4 (Step 000910): Train loss 0.933, Val loss 1.209\n",
      "Ep 4 (Step 000915): Train loss 0.808, Val loss 1.222\n",
      "Ep 4 (Step 000920): Train loss 0.801, Val loss 1.216\n",
      "Ep 4 (Step 000925): Train loss 0.744, Val loss 1.199\n",
      "Ep 4 (Step 000930): Train loss 0.879, Val loss 1.201\n",
      "Ep 4 (Step 000935): Train loss 0.837, Val loss 1.216\n",
      "Ep 4 (Step 000940): Train loss 0.761, Val loss 1.209\n",
      "Ep 4 (Step 000945): Train loss 0.779, Val loss 1.187\n",
      "Ep 4 (Step 000950): Train loss 0.761, Val loss 1.205\n",
      "Every effort moves you to see the                                               \n",
      "Ep 5 (Step 000955): Train loss 0.861, Val loss 1.192\n",
      "Ep 5 (Step 000960): Train loss 0.923, Val loss 1.196\n",
      "Ep 5 (Step 000965): Train loss 0.854, Val loss 1.189\n",
      "Ep 5 (Step 000970): Train loss 0.914, Val loss 1.198\n",
      "Ep 5 (Step 000975): Train loss 0.721, Val loss 1.203\n",
      "Ep 5 (Step 000980): Train loss 0.844, Val loss 1.187\n",
      "Ep 5 (Step 000985): Train loss 0.705, Val loss 1.179\n",
      "Ep 5 (Step 000990): Train loss 0.670, Val loss 1.198\n",
      "Ep 5 (Step 000995): Train loss 0.695, Val loss 1.197\n",
      "Ep 5 (Step 001000): Train loss 0.648, Val loss 1.197\n",
      "Ep 5 (Step 001005): Train loss 0.736, Val loss 1.220\n",
      "Ep 5 (Step 001010): Train loss 0.804, Val loss 1.234\n",
      "Ep 5 (Step 001015): Train loss 0.789, Val loss 1.236\n",
      "Ep 5 (Step 001020): Train loss 0.766, Val loss 1.218\n",
      "Ep 5 (Step 001025): Train loss 0.734, Val loss 1.223\n",
      "Ep 5 (Step 001030): Train loss 0.665, Val loss 1.240\n",
      "Ep 5 (Step 001035): Train loss 0.619, Val loss 1.250\n",
      "Ep 5 (Step 001040): Train loss 0.834, Val loss 1.232\n",
      "Ep 5 (Step 001045): Train loss 0.659, Val loss 1.227\n",
      "Ep 5 (Step 001050): Train loss 0.710, Val loss 1.229\n",
      "Ep 5 (Step 001055): Train loss 0.786, Val loss 1.221\n",
      "Ep 5 (Step 001060): Train loss 0.749, Val loss 1.212\n",
      "Ep 5 (Step 001065): Train loss 0.775, Val loss 1.230\n",
      "Ep 5 (Step 001070): Train loss 0.755, Val loss 1.218\n",
      "Ep 5 (Step 001075): Train loss 0.819, Val loss 1.212\n",
      "Ep 5 (Step 001080): Train loss 0.600, Val loss 1.213\n",
      "Ep 5 (Step 001085): Train loss 0.784, Val loss 1.212\n",
      "Ep 5 (Step 001090): Train loss 0.588, Val loss 1.208\n",
      "Ep 5 (Step 001095): Train loss 0.550, Val loss 1.207\n",
      "Ep 5 (Step 001100): Train loss 0.783, Val loss 1.216\n",
      "Ep 5 (Step 001105): Train loss 0.526, Val loss 1.214\n",
      "Ep 5 (Step 001110): Train loss 0.660, Val loss 1.222\n",
      "Ep 5 (Step 001115): Train loss 0.758, Val loss 1.219\n",
      "Ep 5 (Step 001120): Train loss 0.780, Val loss 1.221\n",
      "Ep 5 (Step 001125): Train loss 0.495, Val loss 1.223\n",
      "Ep 5 (Step 001130): Train loss 0.565, Val loss 1.221\n",
      "Ep 5 (Step 001135): Train loss 0.677, Val loss 1.247\n",
      "Ep 5 (Step 001140): Train loss 0.643, Val loss 1.239\n",
      "Ep 5 (Step 001145): Train loss 0.833, Val loss 1.236\n",
      "Ep 5 (Step 001150): Train loss 0.691, Val loss 1.239\n",
      "Ep 5 (Step 001155): Train loss 0.731, Val loss 1.238\n",
      "Ep 5 (Step 001160): Train loss 0.766, Val loss 1.238\n",
      "Ep 5 (Step 001165): Train loss 0.546, Val loss 1.228\n",
      "Ep 5 (Step 001170): Train loss 0.831, Val loss 1.238\n",
      "Ep 5 (Step 001175): Train loss 0.772, Val loss 1.222\n",
      "Ep 5 (Step 001180): Train loss 0.622, Val loss 1.221\n",
      "Ep 5 (Step 001185): Train loss 0.708, Val loss 1.232\n",
      "Every effort moves you're                                                 \n",
      "Ep 6 (Step 001190): Train loss 0.619, Val loss 1.221\n",
      "Ep 6 (Step 001195): Train loss 0.598, Val loss 1.219\n",
      "Ep 6 (Step 001200): Train loss 0.531, Val loss 1.235\n",
      "Ep 6 (Step 001205): Train loss 0.523, Val loss 1.240\n",
      "Ep 6 (Step 001210): Train loss 0.570, Val loss 1.209\n",
      "Ep 6 (Step 001215): Train loss 0.591, Val loss 1.226\n",
      "Ep 6 (Step 001220): Train loss 0.536, Val loss 1.234\n",
      "Ep 6 (Step 001225): Train loss 0.593, Val loss 1.216\n",
      "Ep 6 (Step 001230): Train loss 0.527, Val loss 1.222\n",
      "Ep 6 (Step 001235): Train loss 0.569, Val loss 1.241\n",
      "Ep 6 (Step 001240): Train loss 0.580, Val loss 1.236\n",
      "Ep 6 (Step 001245): Train loss 0.563, Val loss 1.234\n",
      "Ep 6 (Step 001250): Train loss 0.627, Val loss 1.250\n",
      "Ep 6 (Step 001255): Train loss 0.511, Val loss 1.238\n",
      "Ep 6 (Step 001260): Train loss 0.563, Val loss 1.233\n",
      "Ep 6 (Step 001265): Train loss 0.643, Val loss 1.233\n",
      "Ep 6 (Step 001270): Train loss 0.463, Val loss 1.227\n",
      "Ep 6 (Step 001275): Train loss 0.659, Val loss 1.244\n",
      "Ep 6 (Step 001280): Train loss 0.531, Val loss 1.241\n",
      "Ep 6 (Step 001285): Train loss 0.627, Val loss 1.257\n",
      "Ep 6 (Step 001290): Train loss 0.545, Val loss 1.247\n",
      "Ep 6 (Step 001295): Train loss 0.450, Val loss 1.242\n",
      "Ep 6 (Step 001300): Train loss 0.764, Val loss 1.238\n",
      "Ep 6 (Step 001305): Train loss 0.608, Val loss 1.251\n",
      "Ep 6 (Step 001310): Train loss 0.387, Val loss 1.251\n",
      "Ep 6 (Step 001315): Train loss 0.584, Val loss 1.254\n",
      "Ep 6 (Step 001320): Train loss 0.472, Val loss 1.273\n",
      "Ep 6 (Step 001325): Train loss 0.661, Val loss 1.277\n",
      "Ep 6 (Step 001330): Train loss 0.526, Val loss 1.297\n",
      "Ep 6 (Step 001335): Train loss 0.526, Val loss 1.282\n",
      "Ep 6 (Step 001340): Train loss 0.625, Val loss 1.292\n",
      "Ep 6 (Step 001345): Train loss 0.573, Val loss 1.274\n",
      "Ep 6 (Step 001350): Train loss 0.417, Val loss 1.271\n",
      "Ep 6 (Step 001355): Train loss 0.538, Val loss 1.281\n",
      "Ep 6 (Step 001360): Train loss 0.465, Val loss 1.265\n",
      "Ep 6 (Step 001365): Train loss 0.666, Val loss 1.240\n",
      "Ep 6 (Step 001370): Train loss 0.467, Val loss 1.232\n",
      "Ep 6 (Step 001375): Train loss 0.492, Val loss 1.248\n",
      "Ep 6 (Step 001380): Train loss 0.508, Val loss 1.244\n",
      "Ep 6 (Step 001385): Train loss 0.428, Val loss 1.248\n",
      "Ep 6 (Step 001390): Train loss 0.612, Val loss 1.252\n",
      "Ep 6 (Step 001395): Train loss 0.404, Val loss 1.257\n",
      "Ep 6 (Step 001400): Train loss 0.392, Val loss 1.258\n",
      "Ep 6 (Step 001405): Train loss 0.440, Val loss 1.277\n",
      "Ep 6 (Step 001410): Train loss 0.492, Val loss 1.285\n",
      "Ep 6 (Step 001415): Train loss 0.549, Val loss 1.287\n",
      "Ep 6 (Step 001420): Train loss 0.522, Val loss 1.271\n",
      "Ep 6 (Step 001425): Train loss 0.441, Val loss 1.274\n",
      "Every effort moves you talking about?                                                \n",
      "Ep 7 (Step 001430): Train loss 0.441, Val loss 1.280\n",
      "Ep 7 (Step 001435): Train loss 0.485, Val loss 1.299\n",
      "Ep 7 (Step 001440): Train loss 0.406, Val loss 1.288\n",
      "Ep 7 (Step 001445): Train loss 0.568, Val loss 1.322\n",
      "Ep 7 (Step 001450): Train loss 0.429, Val loss 1.321\n",
      "Ep 7 (Step 001455): Train loss 0.431, Val loss 1.321\n",
      "Ep 7 (Step 001460): Train loss 0.412, Val loss 1.293\n",
      "Ep 7 (Step 001465): Train loss 0.347, Val loss 1.299\n",
      "Ep 7 (Step 001470): Train loss 0.481, Val loss 1.300\n",
      "Ep 7 (Step 001475): Train loss 0.375, Val loss 1.323\n",
      "Ep 7 (Step 001480): Train loss 0.480, Val loss 1.305\n",
      "Ep 7 (Step 001485): Train loss 0.373, Val loss 1.322\n",
      "Ep 7 (Step 001490): Train loss 0.405, Val loss 1.305\n",
      "Ep 7 (Step 001495): Train loss 0.445, Val loss 1.303\n",
      "Ep 7 (Step 001500): Train loss 0.467, Val loss 1.314\n",
      "Ep 7 (Step 001505): Train loss 0.428, Val loss 1.315\n",
      "Ep 7 (Step 001510): Train loss 0.430, Val loss 1.318\n",
      "Ep 7 (Step 001515): Train loss 0.431, Val loss 1.347\n",
      "Ep 7 (Step 001520): Train loss 0.521, Val loss 1.367\n",
      "Ep 7 (Step 001525): Train loss 0.506, Val loss 1.348\n",
      "Ep 7 (Step 001530): Train loss 0.402, Val loss 1.330\n",
      "Ep 7 (Step 001535): Train loss 0.407, Val loss 1.334\n",
      "Ep 7 (Step 001540): Train loss 0.446, Val loss 1.314\n",
      "Ep 7 (Step 001545): Train loss 0.430, Val loss 1.294\n",
      "Ep 7 (Step 001550): Train loss 0.460, Val loss 1.305\n",
      "Ep 7 (Step 001555): Train loss 0.453, Val loss 1.284\n",
      "Ep 7 (Step 001560): Train loss 0.345, Val loss 1.294\n",
      "Ep 7 (Step 001565): Train loss 0.377, Val loss 1.320\n",
      "Ep 7 (Step 001570): Train loss 0.426, Val loss 1.327\n",
      "Ep 7 (Step 001575): Train loss 0.407, Val loss 1.327\n",
      "Ep 7 (Step 001580): Train loss 0.405, Val loss 1.322\n",
      "Ep 7 (Step 001585): Train loss 0.362, Val loss 1.323\n",
      "Ep 7 (Step 001590): Train loss 0.407, Val loss 1.325\n",
      "Ep 7 (Step 001595): Train loss 0.409, Val loss 1.324\n",
      "Ep 7 (Step 001600): Train loss 0.377, Val loss 1.328\n",
      "Ep 7 (Step 001605): Train loss 0.421, Val loss 1.353\n",
      "Ep 7 (Step 001610): Train loss 0.372, Val loss 1.347\n",
      "Ep 7 (Step 001615): Train loss 0.419, Val loss 1.319\n",
      "Ep 7 (Step 001620): Train loss 0.372, Val loss 1.346\n",
      "Ep 7 (Step 001625): Train loss 0.424, Val loss 1.349\n",
      "Ep 7 (Step 001630): Train loss 0.424, Val loss 1.343\n",
      "Ep 7 (Step 001635): Train loss 0.420, Val loss 1.344\n",
      "Ep 7 (Step 001640): Train loss 0.384, Val loss 1.342\n",
      "Ep 7 (Step 001645): Train loss 0.403, Val loss 1.362\n",
      "Ep 7 (Step 001650): Train loss 0.379, Val loss 1.364\n",
      "Ep 7 (Step 001655): Train loss 0.368, Val loss 1.347\n",
      "Ep 7 (Step 001660): Train loss 0.326, Val loss 1.355\n",
      "Ep 7 (Step 001665): Train loss 0.345, Val loss 1.354\n",
      "Every effort moves you, and a CRONY                                             \n",
      "Ep 8 (Step 001670): Train loss 0.324, Val loss 1.352\n",
      "Ep 8 (Step 001675): Train loss 0.473, Val loss 1.362\n",
      "Ep 8 (Step 001680): Train loss 0.421, Val loss 1.352\n",
      "Ep 8 (Step 001685): Train loss 0.374, Val loss 1.369\n",
      "Ep 8 (Step 001690): Train loss 0.421, Val loss 1.380\n",
      "Ep 8 (Step 001695): Train loss 0.384, Val loss 1.365\n",
      "Ep 8 (Step 001700): Train loss 0.332, Val loss 1.351\n",
      "Ep 8 (Step 001705): Train loss 0.352, Val loss 1.369\n",
      "Ep 8 (Step 001710): Train loss 0.382, Val loss 1.382\n",
      "Ep 8 (Step 001715): Train loss 0.356, Val loss 1.374\n",
      "Ep 8 (Step 001720): Train loss 0.341, Val loss 1.355\n",
      "Ep 8 (Step 001725): Train loss 0.377, Val loss 1.379\n",
      "Ep 8 (Step 001730): Train loss 0.361, Val loss 1.394\n",
      "Ep 8 (Step 001735): Train loss 0.331, Val loss 1.379\n",
      "Ep 8 (Step 001740): Train loss 0.375, Val loss 1.369\n",
      "Ep 8 (Step 001745): Train loss 0.371, Val loss 1.383\n",
      "Ep 8 (Step 001750): Train loss 0.334, Val loss 1.368\n",
      "Ep 8 (Step 001755): Train loss 0.283, Val loss 1.360\n",
      "Ep 8 (Step 001760): Train loss 0.307, Val loss 1.363\n",
      "Ep 8 (Step 001765): Train loss 0.331, Val loss 1.388\n",
      "Ep 8 (Step 001770): Train loss 0.410, Val loss 1.370\n",
      "Ep 8 (Step 001775): Train loss 0.353, Val loss 1.382\n",
      "Ep 8 (Step 001780): Train loss 0.389, Val loss 1.393\n",
      "Ep 8 (Step 001785): Train loss 0.314, Val loss 1.372\n",
      "Ep 8 (Step 001790): Train loss 0.326, Val loss 1.370\n",
      "Ep 8 (Step 001795): Train loss 0.329, Val loss 1.362\n",
      "Ep 8 (Step 001800): Train loss 0.334, Val loss 1.382\n",
      "Ep 8 (Step 001805): Train loss 0.414, Val loss 1.376\n",
      "Ep 8 (Step 001810): Train loss 0.362, Val loss 1.355\n",
      "Ep 8 (Step 001815): Train loss 0.363, Val loss 1.363\n",
      "Ep 8 (Step 001820): Train loss 0.308, Val loss 1.353\n",
      "Ep 8 (Step 001825): Train loss 0.305, Val loss 1.347\n",
      "Ep 8 (Step 001830): Train loss 0.294, Val loss 1.356\n",
      "Ep 8 (Step 001835): Train loss 0.339, Val loss 1.363\n",
      "Ep 8 (Step 001840): Train loss 0.266, Val loss 1.363\n",
      "Ep 8 (Step 001845): Train loss 0.307, Val loss 1.367\n",
      "Ep 8 (Step 001850): Train loss 0.260, Val loss 1.372\n",
      "Ep 8 (Step 001855): Train loss 0.263, Val loss 1.362\n",
      "Ep 8 (Step 001860): Train loss 0.265, Val loss 1.365\n",
      "Ep 8 (Step 001865): Train loss 0.237, Val loss 1.347\n",
      "Ep 8 (Step 001870): Train loss 0.283, Val loss 1.332\n",
      "Ep 8 (Step 001875): Train loss 0.313, Val loss 1.325\n",
      "Ep 8 (Step 001880): Train loss 0.267, Val loss 1.346\n",
      "Ep 8 (Step 001885): Train loss 0.289, Val loss 1.365\n",
      "Ep 8 (Step 001890): Train loss 0.246, Val loss 1.375\n",
      "Ep 8 (Step 001895): Train loss 0.278, Val loss 1.372\n",
      "Ep 8 (Step 001900): Train loss 0.352, Val loss 1.383\n",
      "Every effort moves you the door, his lap.                                             \n",
      "Ep 9 (Step 001905): Train loss 0.338, Val loss 1.396\n",
      "Ep 9 (Step 001910): Train loss 0.299, Val loss 1.400\n",
      "Ep 9 (Step 001915): Train loss 0.318, Val loss 1.405\n",
      "Ep 9 (Step 001920): Train loss 0.332, Val loss 1.419\n",
      "Ep 9 (Step 001925): Train loss 0.277, Val loss 1.441\n",
      "Ep 9 (Step 001930): Train loss 0.303, Val loss 1.466\n",
      "Ep 9 (Step 001935): Train loss 0.243, Val loss 1.425\n",
      "Ep 9 (Step 001940): Train loss 0.304, Val loss 1.411\n",
      "Ep 9 (Step 001945): Train loss 0.335, Val loss 1.430\n",
      "Ep 9 (Step 001950): Train loss 0.351, Val loss 1.456\n",
      "Ep 9 (Step 001955): Train loss 0.278, Val loss 1.410\n",
      "Ep 9 (Step 001960): Train loss 0.395, Val loss 1.412\n",
      "Ep 9 (Step 001965): Train loss 0.340, Val loss 1.422\n",
      "Ep 9 (Step 001970): Train loss 0.303, Val loss 1.427\n",
      "Ep 9 (Step 001975): Train loss 0.334, Val loss 1.411\n",
      "Ep 9 (Step 001980): Train loss 0.225, Val loss 1.385\n",
      "Ep 9 (Step 001985): Train loss 0.263, Val loss 1.393\n",
      "Ep 9 (Step 001990): Train loss 0.370, Val loss 1.408\n",
      "Ep 9 (Step 001995): Train loss 0.304, Val loss 1.398\n",
      "Ep 9 (Step 002000): Train loss 0.263, Val loss 1.395\n",
      "Ep 9 (Step 002005): Train loss 0.274, Val loss 1.400\n",
      "Ep 9 (Step 002010): Train loss 0.261, Val loss 1.388\n",
      "Ep 9 (Step 002015): Train loss 0.289, Val loss 1.424\n",
      "Ep 9 (Step 002020): Train loss 0.405, Val loss 1.417\n",
      "Ep 9 (Step 002025): Train loss 0.334, Val loss 1.392\n",
      "Ep 9 (Step 002030): Train loss 0.251, Val loss 1.388\n",
      "Ep 9 (Step 002035): Train loss 0.343, Val loss 1.399\n",
      "Ep 9 (Step 002040): Train loss 0.341, Val loss 1.441\n",
      "Ep 9 (Step 002045): Train loss 0.326, Val loss 1.444\n",
      "Ep 9 (Step 002050): Train loss 0.436, Val loss 1.467\n",
      "Ep 9 (Step 002055): Train loss 0.386, Val loss 1.443\n",
      "Ep 9 (Step 002060): Train loss 0.314, Val loss 1.430\n",
      "Ep 9 (Step 002065): Train loss 0.343, Val loss 1.403\n",
      "Ep 9 (Step 002070): Train loss 0.293, Val loss 1.413\n",
      "Ep 9 (Step 002075): Train loss 0.267, Val loss 1.391\n",
      "Ep 9 (Step 002080): Train loss 0.323, Val loss 1.387\n",
      "Ep 9 (Step 002085): Train loss 0.317, Val loss 1.371\n",
      "Ep 9 (Step 002090): Train loss 0.263, Val loss 1.373\n",
      "Ep 9 (Step 002095): Train loss 0.369, Val loss 1.388\n",
      "Ep 9 (Step 002100): Train loss 0.295, Val loss 1.393\n",
      "Ep 9 (Step 002105): Train loss 0.243, Val loss 1.380\n",
      "Ep 9 (Step 002110): Train loss 0.243, Val loss 1.403\n",
      "Ep 9 (Step 002115): Train loss 0.268, Val loss 1.387\n",
      "Ep 9 (Step 002120): Train loss 0.249, Val loss 1.392\n",
      "Ep 9 (Step 002125): Train loss 0.274, Val loss 1.386\n",
      "Ep 9 (Step 002130): Train loss 0.273, Val loss 1.398\n",
      "Ep 9 (Step 002135): Train loss 0.322, Val loss 1.387\n",
      "Ep 9 (Step 002140): Train loss 0.243, Val loss 1.414\n",
      "Every effort moves you the door, and                                the door.            \n",
      "Ep 10 (Step 002145): Train loss 0.256, Val loss 1.431\n",
      "Ep 10 (Step 002150): Train loss 0.241, Val loss 1.435\n",
      "Ep 10 (Step 002155): Train loss 0.296, Val loss 1.413\n",
      "Ep 10 (Step 002160): Train loss 0.275, Val loss 1.467\n",
      "Ep 10 (Step 002165): Train loss 0.243, Val loss 1.463\n",
      "Ep 10 (Step 002170): Train loss 0.262, Val loss 1.476\n",
      "Ep 10 (Step 002175): Train loss 0.297, Val loss 1.438\n",
      "Ep 10 (Step 002180): Train loss 0.305, Val loss 1.432\n",
      "Ep 10 (Step 002185): Train loss 0.239, Val loss 1.425\n",
      "Ep 10 (Step 002190): Train loss 0.302, Val loss 1.410\n",
      "Ep 10 (Step 002195): Train loss 0.264, Val loss 1.435\n",
      "Ep 10 (Step 002200): Train loss 0.315, Val loss 1.423\n",
      "Ep 10 (Step 002205): Train loss 0.346, Val loss 1.443\n",
      "Ep 10 (Step 002210): Train loss 0.263, Val loss 1.408\n",
      "Ep 10 (Step 002215): Train loss 0.227, Val loss 1.419\n",
      "Ep 10 (Step 002220): Train loss 0.263, Val loss 1.421\n",
      "Ep 10 (Step 002225): Train loss 0.229, Val loss 1.454\n",
      "Ep 10 (Step 002230): Train loss 0.292, Val loss 1.431\n",
      "Ep 10 (Step 002235): Train loss 0.245, Val loss 1.426\n",
      "Ep 10 (Step 002240): Train loss 0.285, Val loss 1.410\n",
      "Ep 10 (Step 002245): Train loss 0.258, Val loss 1.421\n",
      "Ep 10 (Step 002250): Train loss 0.214, Val loss 1.458\n",
      "Ep 10 (Step 002255): Train loss 0.269, Val loss 1.433\n",
      "Ep 10 (Step 002260): Train loss 0.251, Val loss 1.440\n",
      "Ep 10 (Step 002265): Train loss 0.252, Val loss 1.435\n",
      "Ep 10 (Step 002270): Train loss 0.193, Val loss 1.435\n",
      "Ep 10 (Step 002275): Train loss 0.246, Val loss 1.453\n",
      "Ep 10 (Step 002280): Train loss 0.250, Val loss 1.442\n",
      "Ep 10 (Step 002285): Train loss 0.229, Val loss 1.448\n",
      "Ep 10 (Step 002290): Train loss 0.228, Val loss 1.420\n",
      "Ep 10 (Step 002295): Train loss 0.230, Val loss 1.427\n",
      "Ep 10 (Step 002300): Train loss 0.259, Val loss 1.423\n",
      "Ep 10 (Step 002305): Train loss 0.211, Val loss 1.445\n",
      "Ep 10 (Step 002310): Train loss 0.236, Val loss 1.465\n",
      "Ep 10 (Step 002315): Train loss 0.202, Val loss 1.486\n",
      "Ep 10 (Step 002320): Train loss 0.233, Val loss 1.429\n",
      "Ep 10 (Step 002325): Train loss 0.266, Val loss 1.416\n",
      "Ep 10 (Step 002330): Train loss 0.218, Val loss 1.430\n",
      "Ep 10 (Step 002335): Train loss 0.273, Val loss 1.442\n",
      "Ep 10 (Step 002340): Train loss 0.208, Val loss 1.461\n",
      "Ep 10 (Step 002345): Train loss 0.195, Val loss 1.445\n",
      "Ep 10 (Step 002350): Train loss 0.184, Val loss 1.448\n",
      "Ep 10 (Step 002355): Train loss 0.215, Val loss 1.432\n",
      "Ep 10 (Step 002360): Train loss 0.200, Val loss 1.455\n",
      "Ep 10 (Step 002365): Train loss 0.269, Val loss 1.451\n",
      "Ep 10 (Step 002370): Train loss 0.194, Val loss 1.443\n",
      "Ep 10 (Step 002375): Train loss 0.167, Val loss 1.428\n",
      "Every effort moves you the                                                 \n"
     ]
    }
   ],
   "execution_count": 32
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-04T13:44:20.609051Z",
     "start_time": "2025-06-04T13:44:19.869948Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "def plot_losses(epochs_seen, tokens_seen, train_losses, val_losses):\n",
    "    fig, ax1 = plt.subplots(figsize=(5, 3))\n",
    "    ax1.plot(epochs_seen, train_losses, label=\"Training loss\")\n",
    "    ax1.plot(\n",
    "        epochs_seen, val_losses, linestyle=\"-.\", label=\"Validation loss\"\n",
    "    )\n",
    "    ax1.set_xlabel(\"Epochs\")\n",
    "    ax1.set_ylabel(\"Loss\")\n",
    "    ax1.legend(loc=\"upper right\")\n",
    "    ax1.xaxis.set_major_locator(MaxNLocator(integer=True))\n",
    "    ax2 = ax1.twiny()                   #1\n",
    "    ax2.plot(tokens_seen, train_losses, alpha=0)     #2\n",
    "    ax2.set_xlabel(\"Tokens seen\")\n",
    "    fig.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "epochs_tensor = torch.linspace(0, num_epochs, len(train_losses))\n",
    "plot_losses(epochs_tensor, tokens_seen, train_losses, val_losses)"
   ],
   "id": "b74662c1d5a780a6",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 500x300 with 2 Axes>"
      ],
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 34
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-04T13:44:23.271838Z",
     "start_time": "2025-06-04T13:44:22.738637Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model.to(\"cpu\")\n",
    "model.eval()"
   ],
   "id": "a0ead31e7e1542cb",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPTModel(\n",
       "  (tok_emb): Embedding(50257, 768)\n",
       "  (pos_emb): Embedding(256, 768)\n",
       "  (drop_emb): Dropout(p=0.1, inplace=False)\n",
       "  (trf_blocks): Sequential(\n",
       "    (0): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (1): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (2): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (3): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (4): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (5): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (6): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (7): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (8): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (9): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (10): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (11): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (final_norm): LayerNorm()\n",
       "  (out_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 35
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-04T13:44:40.480787Z",
     "start_time": "2025-06-04T13:44:39.556147Z"
    }
   },
   "cell_type": "code",
   "source": [
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "token_ids = generate_text_simple(\n",
    "    model=model,\n",
    "    idx=text_to_token_ids(\"Every effort moves you\", tokenizer),\n",
    "    max_new_tokens=25,\n",
    "    context_size=GPT_CONFIG_124M[\"context_length\"]\n",
    ")\n",
    "print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))"
   ],
   "id": "75c4482e3abdb1da",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output text:\n",
      " Every effort moves you the\n",
      "                       \n"
     ]
    }
   ],
   "execution_count": 36
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-04T13:44:42.067631Z",
     "start_time": "2025-06-04T13:44:42.065490Z"
    }
   },
   "cell_type": "code",
   "source": [
    "vocab = {\n",
    "    \"closer\": 0,\n",
    "    \"every\": 1,\n",
    "    \"effort\": 2,\n",
    "    \"forward\": 3,\n",
    "    \"inches\": 4,\n",
    "    \"moves\": 5,\n",
    "    \"pizza\": 6,\n",
    "    \"toward\": 7,\n",
    "    \"you\": 8,\n",
    "}\n",
    "inverse_vocab = {v: k for k, v in vocab.items()}"
   ],
   "id": "a66178201b463a91",
   "outputs": [],
   "execution_count": 37
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-04T13:44:43.982693Z",
     "start_time": "2025-06-04T13:44:43.980753Z"
    }
   },
   "cell_type": "code",
   "source": [
    "next_token_logits = torch.tensor(\n",
    "    [4.51, 0.89, -1.90, 6.75, 1.63, -1.62, -1.89, 6.28, 1.79]\n",
    ")"
   ],
   "id": "5d5469a52135e7a4",
   "outputs": [],
   "execution_count": 38
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-04T13:44:45.189703Z",
     "start_time": "2025-06-04T13:44:45.187504Z"
    }
   },
   "cell_type": "code",
   "source": [
    "probas = torch.softmax(next_token_logits, dim=0)\n",
    "next_token_id = torch.argmax(probas).item()\n",
    "print(inverse_vocab[next_token_id])"
   ],
   "id": "4324b867b5901f04",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "forward\n"
     ]
    }
   ],
   "execution_count": 39
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-04T13:44:46.520076Z",
     "start_time": "2025-06-04T13:44:46.517187Z"
    }
   },
   "cell_type": "code",
   "source": [
    "torch.manual_seed(123)\n",
    "next_token_id = torch.multinomial(probas, num_samples=1).item()\n",
    "print(inverse_vocab[next_token_id])"
   ],
   "id": "b4ce6b022201d6d6",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "forward\n"
     ]
    }
   ],
   "execution_count": 40
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-04T13:44:47.826721Z",
     "start_time": "2025-06-04T13:44:47.812291Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def print_sampled_tokens(probas):\n",
    "    torch.manual_seed(123)\n",
    "    sample = [torch.multinomial(probas, num_samples=1).item()\n",
    "             for i in range(1_000)]\n",
    "    sampled_ids = torch.bincount(torch.tensor(sample))\n",
    "    for i, freq in enumerate(sampled_ids):\n",
    "        print(f\"{freq} x {inverse_vocab[i]}\")\n",
    "\n",
    "print_sampled_tokens(probas)"
   ],
   "id": "be4258ff91031425",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "73 x closer\n",
      "0 x every\n",
      "0 x effort\n",
      "582 x forward\n",
      "2 x inches\n",
      "0 x moves\n",
      "0 x pizza\n",
      "343 x toward\n"
     ]
    }
   ],
   "execution_count": 41
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-04T13:44:49.530069Z",
     "start_time": "2025-06-04T13:44:49.528217Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def softmax_with_temperature(logits, temperature):\n",
    "    scaled_logits = logits / temperature\n",
    "    return torch.softmax(scaled_logits, dim=0)"
   ],
   "id": "a33dc66095530c52",
   "outputs": [],
   "execution_count": 42
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-04T13:44:50.832864Z",
     "start_time": "2025-06-04T13:44:50.763228Z"
    }
   },
   "cell_type": "code",
   "source": [
    "temperatures = [1, 0.1, 5]                                     #1\n",
    "scaled_probas = [softmax_with_temperature(next_token_logits, T)\n",
    "                for T in temperatures]\n",
    "x = torch.arange(len(vocab))\n",
    "bar_width = 0.15\n",
    "fig, ax = plt.subplots(figsize=(5, 3))\n",
    "for i, T in enumerate(temperatures):\n",
    "    rects = ax.bar(x + i * bar_width, scaled_probas[i],\n",
    "                   bar_width, label=f'Temperature = {T}')\n",
    "ax.set_ylabel('Probability')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(vocab.keys(), rotation=90)\n",
    "ax.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "6ac62d8895e8f017",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 500x300 with 1 Axes>"
      ],
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 43
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-04T13:45:00.615788Z",
     "start_time": "2025-06-04T13:45:00.613091Z"
    }
   },
   "cell_type": "code",
   "source": [
    "top_k = 3\n",
    "top_logits, top_pos = torch.topk(next_token_logits, top_k)\n",
    "print(\"Top logits:\", top_logits)\n",
    "print(\"Top positions:\", top_pos)"
   ],
   "id": "b22b5d30c64b6a36",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top logits: tensor([6.7500, 6.2800, 4.5100])\n",
      "Top positions: tensor([3, 7, 0])\n"
     ]
    }
   ],
   "execution_count": 44
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-04T13:45:01.695658Z",
     "start_time": "2025-06-04T13:45:01.692989Z"
    }
   },
   "cell_type": "code",
   "source": [
    "new_logits = torch.where(\n",
    "    condition=next_token_logits < top_logits[-1],    #1\n",
    "    input=torch.tensor(float('-inf')),     #2\n",
    "    other=next_token_logits     #3\n",
    ")\n",
    "print(new_logits)"
   ],
   "id": "66db1e6f0f34dc83",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([4.5100,   -inf,   -inf, 6.7500,   -inf,   -inf,   -inf, 6.2800,   -inf])\n"
     ]
    }
   ],
   "execution_count": 45
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-04T13:45:02.988967Z",
     "start_time": "2025-06-04T13:45:02.986440Z"
    }
   },
   "cell_type": "code",
   "source": [
    "topk_probas = torch.softmax(new_logits, dim=0)\n",
    "print(topk_probas)"
   ],
   "id": "10c0d4ec5965607d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.0615, 0.0000, 0.0000, 0.5775, 0.0000, 0.0000, 0.0000, 0.3610, 0.0000])\n"
     ]
    }
   ],
   "execution_count": 46
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-04T13:45:04.237785Z",
     "start_time": "2025-06-04T13:45:04.234861Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def generate(model, idx, max_new_tokens, context_size,\n",
    "             temperature=0.0, top_k=None, eos_id=None):\n",
    "    for _ in range(max_new_tokens):            #1\n",
    "        idx_cond = idx[:, -context_size:]\n",
    "        with torch.no_grad():\n",
    "            logits = model(idx_cond)\n",
    "        logits = logits[:, -1, :]\n",
    "        if top_k is not None:                #2\n",
    "            top_logits, _ = torch.topk(logits, top_k)\n",
    "            min_val = top_logits[:, -1]\n",
    "            logits = torch.where(\n",
    "                logits < min_val,\n",
    "                torch.tensor(float('-inf')).to(logits.device),\n",
    "                logits\n",
    "            )\n",
    "        if temperature > 0.0:                  #3\n",
    "            logits = logits / temperature\n",
    "            probs = torch.softmax(logits, dim=-1)\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)\n",
    "        else:    #4\n",
    "            idx_next = torch.argmax(logits, dim=-1, keepdim=True)\n",
    "        if idx_next == eos_id:              #5\n",
    "            break\n",
    "        idx = torch.cat((idx, idx_next), dim=1)\n",
    "    return idx"
   ],
   "id": "45a8a5b89375116b",
   "outputs": [],
   "execution_count": 47
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-04T13:45:06.855441Z",
     "start_time": "2025-06-04T13:45:06.348877Z"
    }
   },
   "cell_type": "code",
   "source": [
    "torch.manual_seed(123)\n",
    "token_ids = generate(\n",
    "    model=model,\n",
    "    idx=text_to_token_ids(\"Every effort moves you\", tokenizer),\n",
    "    max_new_tokens=15,\n",
    "    context_size=GPT_CONFIG_124M[\"context_length\"],\n",
    "    top_k=25,\n",
    "    temperature=1.4\n",
    ")\n",
    "print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))"
   ],
   "id": "5796bdd1f5da1c9b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output text:\n",
      " Every effort moves you the jury.  The legalIVE JURORS' is\n",
      "  \n"
     ]
    }
   ],
   "execution_count": 48
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-04T13:45:26.874737Z",
     "start_time": "2025-06-04T13:45:26.381456Z"
    }
   },
   "cell_type": "code",
   "source": "torch.save(model.state_dict(), \"model.pth\")\n",
   "id": "f0f00fe8be3f25b7",
   "outputs": [],
   "execution_count": 49
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-04T13:45:29.230933Z",
     "start_time": "2025-06-04T13:45:28.347138Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "model.load_state_dict(torch.load(\"model.pth\", map_location=device))\n",
    "model.eval()"
   ],
   "id": "40b083d59c514ef2",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPTModel(\n",
       "  (tok_emb): Embedding(50257, 768)\n",
       "  (pos_emb): Embedding(256, 768)\n",
       "  (drop_emb): Dropout(p=0.1, inplace=False)\n",
       "  (trf_blocks): Sequential(\n",
       "    (0): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (1): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (2): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (3): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (4): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (5): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (6): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (7): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (8): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (9): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (10): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (11): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (final_norm): LayerNorm()\n",
       "  (out_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 50
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-04T13:45:32.922791Z",
     "start_time": "2025-06-04T13:45:30.884352Z"
    }
   },
   "cell_type": "code",
   "source": [
    "torch.save({\n",
    "    \"model_state_dict\": model.state_dict(),\n",
    "    \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "    },\n",
    "    \"model_and_optimizer.pth\"\n",
    ")"
   ],
   "id": "b9eb4092dbd0d435",
   "outputs": [],
   "execution_count": 51
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-04T13:45:35.756055Z",
     "start_time": "2025-06-04T13:45:33.958453Z"
    }
   },
   "cell_type": "code",
   "source": [
    "checkpoint = torch.load(\"model_and_optimizer.pth\", map_location=device)\n",
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-4, weight_decay=0.1)\n",
    "optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])\n",
    "model.train();"
   ],
   "id": "1d81ee890c969e27",
   "outputs": [],
   "execution_count": 52
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-04T13:51:09.027047Z",
     "start_time": "2025-06-04T13:51:08.669383Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import urllib.request\n",
    "url = (\n",
    "    \"https://raw.githubusercontent.com/rasbt/\"\n",
    "    \"LLMs-from-scratch/main/ch05/\"\n",
    "    \"01_main-chapter-code/gpt_download.py\"\n",
    ")\n",
    "filename = url.split('/')[-1]\n",
    "urllib.request.urlretrieve(url, filename)"
   ],
   "id": "4ccf226f79a3c80e",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('gpt_download.py', <http.client.HTTPMessage at 0x7af95c0401a0>)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 56
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-04T13:53:04.102305Z",
     "start_time": "2025-06-04T13:51:10.313714Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from gpt_download import download_and_load_gpt2\n",
    "settings, params = download_and_load_gpt2(\n",
    "    model_size=\"124M\", models_dir=\"gpt2\"\n",
    ")"
   ],
   "id": "55b114062fe1a666",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-04 21:51:10.468348: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-06-04 21:51:10.475828: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1749045070.485527 2379134 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1749045070.488217 2379134 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1749045070.496131 2379134 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1749045070.496144 2379134 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1749045070.496145 2379134 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1749045070.496146 2379134 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-06-04 21:51:10.498721: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "checkpoint: 100%|██████████| 77.0/77.0 [00:00<00:00, 304kiB/s]\n",
      "encoder.json: 100%|██████████| 1.04M/1.04M [00:01<00:00, 817kiB/s] \n",
      "hparams.json: 100%|██████████| 90.0/90.0 [00:00<00:00, 377kiB/s]\n",
      "model.ckpt.data-00000-of-00001: 100%|██████████| 498M/498M [01:41<00:00, 4.91MiB/s]   \n",
      "model.ckpt.index: 100%|██████████| 5.21k/5.21k [00:00<00:00, 18.9MiB/s]\n",
      "model.ckpt.meta: 100%|██████████| 471k/471k [00:00<00:00, 576kiB/s]  \n",
      "vocab.bpe: 100%|██████████| 456k/456k [00:01<00:00, 446kiB/s]  \n"
     ]
    }
   ],
   "execution_count": 57
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-04T13:53:08.808545Z",
     "start_time": "2025-06-04T13:53:08.806313Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(\"Settings:\", settings)\n",
    "print(\"Parameter dictionary keys:\", params.keys())"
   ],
   "id": "b505a1b4d81acc79",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Settings: {'n_vocab': 50257, 'n_ctx': 1024, 'n_embd': 768, 'n_head': 12, 'n_layer': 12}\n",
      "Parameter dictionary keys: dict_keys(['blocks', 'b', 'g', 'wpe', 'wte'])\n"
     ]
    }
   ],
   "execution_count": 58
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-04T13:53:10.158486Z",
     "start_time": "2025-06-04T13:53:10.155956Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(params[\"wte\"])\n",
    "print(\"Token embedding weight tensor dimensions:\", params[\"wte\"].shape)"
   ],
   "id": "933d29d12c016138",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.11010301 -0.03926672  0.03310751 ... -0.1363697   0.01506208\n",
      "   0.04531523]\n",
      " [ 0.04034033 -0.04861503  0.04624869 ...  0.08605453  0.00253983\n",
      "   0.04318958]\n",
      " [-0.12746179  0.04793796  0.18410145 ...  0.08991534 -0.12972379\n",
      "  -0.08785918]\n",
      " ...\n",
      " [-0.04453601 -0.05483596  0.01225674 ...  0.10435229  0.09783269\n",
      "  -0.06952604]\n",
      " [ 0.1860082   0.01665728  0.04611587 ... -0.09625227  0.07847701\n",
      "  -0.02245961]\n",
      " [ 0.05135201 -0.02768905  0.0499369  ...  0.00704835  0.15519823\n",
      "   0.12067825]]\n",
      "Token embedding weight tensor dimensions: (50257, 768)\n"
     ]
    }
   ],
   "execution_count": 59
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-04T13:53:12.095319Z",
     "start_time": "2025-06-04T13:53:12.093339Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model_configs = {\n",
    "    \"gpt2-small (124M)\": {\"emb_dim\": 768, \"n_layers\": 12, \"n_heads\": 12},\n",
    "    \"gpt2-medium (355M)\": {\"emb_dim\": 1024, \"n_layers\": 24, \"n_heads\": 16},\n",
    "    \"gpt2-large (774M)\": {\"emb_dim\": 1280, \"n_layers\": 36, \"n_heads\": 20},\n",
    "    \"gpt2-xl (1558M)\": {\"emb_dim\": 1600, \"n_layers\": 48, \"n_heads\": 25},\n",
    "}"
   ],
   "id": "7cff40df45d12a44",
   "outputs": [],
   "execution_count": 60
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-04T13:56:49.237204Z",
     "start_time": "2025-06-04T13:56:49.235483Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model_name = \"gpt2-small (124M)\"\n",
    "NEW_CONFIG = GPT_CONFIG_124M.copy()\n",
    "NEW_CONFIG.update(model_configs[model_name])"
   ],
   "id": "68fbac13e13fa99e",
   "outputs": [],
   "execution_count": 93
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-04T13:56:50.161123Z",
     "start_time": "2025-06-04T13:56:50.159522Z"
    }
   },
   "cell_type": "code",
   "source": "NEW_CONFIG.update({\"context_length\": 1024})\n",
   "id": "a85c2aff45c84cac",
   "outputs": [],
   "execution_count": 94
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-04T13:56:50.898257Z",
     "start_time": "2025-06-04T13:56:50.896636Z"
    }
   },
   "cell_type": "code",
   "source": "NEW_CONFIG.update({\"qkv_bias\": True})",
   "id": "25c5fb666a6f86e",
   "outputs": [],
   "execution_count": 95
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-04T13:56:52.131527Z",
     "start_time": "2025-06-04T13:56:51.536171Z"
    }
   },
   "cell_type": "code",
   "source": [
    "gpt = GPTModel(NEW_CONFIG)\n",
    "gpt.eval()"
   ],
   "id": "2f1ed2cfe9e3e193",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPTModel(\n",
       "  (tok_emb): Embedding(50257, 768)\n",
       "  (pos_emb): Embedding(1024, 768)\n",
       "  (drop_emb): Dropout(p=0.1, inplace=False)\n",
       "  (trf_blocks): Sequential(\n",
       "    (0): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (1): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (2): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (3): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (4): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (5): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (6): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (7): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (8): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (9): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (10): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (11): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (final_norm): LayerNorm()\n",
       "  (out_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 96
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-04T13:56:53.507502Z",
     "start_time": "2025-06-04T13:56:53.505489Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def assign(left, right):\n",
    "    if left.shape != right.shape:\n",
    "        raise ValueError(f\"Shape mismatch. Left: {left.shape}, \"\n",
    "                          \"Right: {right.shape}\"\n",
    "        )\n",
    "    return torch.nn.Parameter(torch.tensor(right))"
   ],
   "id": "72fd7f1ce07da932",
   "outputs": [],
   "execution_count": 97
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-04T13:56:54.346617Z",
     "start_time": "2025-06-04T13:56:54.341707Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "\n",
    "def load_weights_into_gpt(gpt, params):           #1\n",
    "    gpt.pos_emb.weight = assign(gpt.pos_emb.weight, params['wpe'])\n",
    "    gpt.tok_emb.weight = assign(gpt.tok_emb.weight, params['wte'])\n",
    "\n",
    "    for b in range(len(params[\"blocks\"])):     #2\n",
    "        q_w, k_w, v_w = np.split(                            #3\n",
    "            (params[\"blocks\"][b][\"attn\"][\"c_attn\"])[\"w\"], 3, axis=-1)\n",
    "        gpt.trf_blocks[b].att.W_query.weight = assign(\n",
    "            gpt.trf_blocks[b].att.W_query.weight, q_w.T)\n",
    "        gpt.trf_blocks[b].att.W_key.weight = assign(\n",
    "            gpt.trf_blocks[b].att.W_key.weight, k_w.T)\n",
    "        gpt.trf_blocks[b].att.W_value.weight = assign(\n",
    "            gpt.trf_blocks[b].att.W_value.weight, v_w.T)\n",
    "\n",
    "        q_b, k_b, v_b = np.split(\n",
    "            (params[\"blocks\"][b][\"attn\"][\"c_attn\"])[\"b\"], 3, axis=-1)\n",
    "        gpt.trf_blocks[b].att.W_query.bias = assign(\n",
    "            gpt.trf_blocks[b].att.W_query.bias, q_b)\n",
    "        gpt.trf_blocks[b].att.W_key.bias = assign(\n",
    "            gpt.trf_blocks[b].att.W_key.bias, k_b)\n",
    "        gpt.trf_blocks[b].att.W_value.bias = assign(\n",
    "            gpt.trf_blocks[b].att.W_value.bias, v_b)\n",
    "\n",
    "        gpt.trf_blocks[b].att.out_proj.weight = assign(\n",
    "            gpt.trf_blocks[b].att.out_proj.weight,\n",
    "            params[\"blocks\"][b][\"attn\"][\"c_proj\"][\"w\"].T)\n",
    "        gpt.trf_blocks[b].att.out_proj.bias = assign(\n",
    "            gpt.trf_blocks[b].att.out_proj.bias,\n",
    "            params[\"blocks\"][b][\"attn\"][\"c_proj\"][\"b\"])\n",
    "\n",
    "        gpt.trf_blocks[b].ff.layers[0].weight = assign(\n",
    "            gpt.trf_blocks[b].ff.layers[0].weight,\n",
    "            params[\"blocks\"][b][\"mlp\"][\"c_fc\"][\"w\"].T)\n",
    "        gpt.trf_blocks[b].ff.layers[0].bias = assign(\n",
    "            gpt.trf_blocks[b].ff.layers[0].bias,\n",
    "            params[\"blocks\"][b][\"mlp\"][\"c_fc\"][\"b\"])\n",
    "        gpt.trf_blocks[b].ff.layers[2].weight = assign(\n",
    "            gpt.trf_blocks[b].ff.layers[2].weight,\n",
    "            params[\"blocks\"][b][\"mlp\"][\"c_proj\"][\"w\"].T)\n",
    "        gpt.trf_blocks[b].ff.layers[2].bias = assign(\n",
    "            gpt.trf_blocks[b].ff.layers[2].bias,\n",
    "            params[\"blocks\"][b][\"mlp\"][\"c_proj\"][\"b\"])\n",
    "\n",
    "        gpt.trf_blocks[b].norm1.scale = assign(\n",
    "            gpt.trf_blocks[b].norm1.scale,\n",
    "            params[\"blocks\"][b][\"ln_1\"][\"g\"])\n",
    "        gpt.trf_blocks[b].norm1.shift = assign(\n",
    "            gpt.trf_blocks[b].norm1.shift,\n",
    "            params[\"blocks\"][b][\"ln_1\"][\"b\"])\n",
    "        gpt.trf_blocks[b].norm2.scale = assign(\n",
    "            gpt.trf_blocks[b].norm2.scale,\n",
    "            params[\"blocks\"][b][\"ln_2\"][\"g\"])\n",
    "        gpt.trf_blocks[b].norm2.shift = assign(\n",
    "            gpt.trf_blocks[b].norm2.shift,\n",
    "            params[\"blocks\"][b][\"ln_2\"][\"b\"])\n",
    "\n",
    "    gpt.final_norm.scale = assign(gpt.final_norm.scale, params[\"g\"])\n",
    "    gpt.final_norm.shift = assign(gpt.final_norm.shift, params[\"b\"])\n",
    "    gpt.out_head.weight = assign(gpt.out_head.weight, params[\"wte\"])    #4"
   ],
   "id": "505b8d6f3a9c44a4",
   "outputs": [],
   "execution_count": 98
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-04T13:56:56.459226Z",
     "start_time": "2025-06-04T13:56:56.287211Z"
    }
   },
   "cell_type": "code",
   "source": [
    "load_weights_into_gpt(gpt, params)\n",
    "gpt.to(device)"
   ],
   "id": "508a7e140da491e8",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPTModel(\n",
       "  (tok_emb): Embedding(50257, 768)\n",
       "  (pos_emb): Embedding(1024, 768)\n",
       "  (drop_emb): Dropout(p=0.1, inplace=False)\n",
       "  (trf_blocks): Sequential(\n",
       "    (0): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (1): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (2): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (3): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (4): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (5): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (6): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (7): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (8): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (9): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (10): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (11): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (final_norm): LayerNorm()\n",
       "  (out_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 99
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-04T13:56:58.310195Z",
     "start_time": "2025-06-04T13:56:58.161296Z"
    }
   },
   "cell_type": "code",
   "source": [
    "torch.manual_seed(123)\n",
    "token_ids = generate(\n",
    "    model=gpt,\n",
    "    idx=text_to_token_ids(\"Every effort moves you\", tokenizer).to(device),\n",
    "    max_new_tokens=25,\n",
    "    context_size=NEW_CONFIG[\"context_length\"],\n",
    "    top_k=50,\n",
    "    temperature=1.5\n",
    ")\n",
    "print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))"
   ],
   "id": "6dd1d6b0607b44d9",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output text:\n",
      " Every effort moves you as far as the hand can go until the end of your turn unless something interrupts your control flow. As you may observe I\n"
     ]
    }
   ],
   "execution_count": 100
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "92f52eca81e2aff0"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
